# ğŸ” AUDITORIA COMPLETA - Database & Infrastructure

**Data**: 06/11/2025  
**Status**: âš ï¸ **AÃ‡Ã•ES NECESSÃRIAS IDENTIFICADAS**

---

## ğŸ“Š **ANÃLISE DA ESTRUTURA**

### **1. Pasta `/database` (Raiz do Projeto)**

```
database/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pg_hba_extra.conf    âœ… ConfiguraÃ§Ã£o segura (md5)
â””â”€â”€ init/
    â””â”€â”€ init_alembic.py       âš ï¸ Script bÃ¡sico, falta integraÃ§Ã£o
```

#### **âœ… Pontos Positivos:**
- pg_hba_extra.conf bem configurado para produÃ§Ã£o (md5, nÃ£o trust)
- Suporte a redes Docker (172.16.0.0/12, 10.0.0.0/8)
- ConfiguraÃ§Ãµes de replicaÃ§Ã£o presentes

#### **âš ï¸ Melhorias NecessÃ¡rias:**

**1.1. init_alembic.py estÃ¡ incompleto**
- âŒ NÃ£o cria arquivo `alembic.ini`
- âŒ NÃ£o configura `env.py`
- âŒ NÃ£o aponta para os modelos

**RECOMENDAÃ‡ÃƒO**: Substituir por script completo ou usar `alembic init`

---

### **2. Pasta `/backend/database`**

```
backend/database/
â”œâ”€â”€ connection.py          âœ… ConfiguraÃ§Ã£o robusta
â”œâ”€â”€ data_storage.py        âš ï¸ Modelo desatualizado
â”œâ”€â”€ health_checks.py       âœ… Monitoramento completo
â”œâ”€â”€ redis_pool.py          (nÃ£o analisado ainda)
â”œâ”€â”€ session_database.py    (nÃ£o analisado ainda)
â””â”€â”€ models/
    â”œâ”€â”€ admin_user.py
    â”œâ”€â”€ climate_data.py    âŒ NÃƒO ESTÃ SENDO USADO!
    â”œâ”€â”€ user_cache.py
    â”œâ”€â”€ user_favorites.py
    â””â”€â”€ visitor_stats.py
```

#### **âœ… Pontos Positivos:**
- `connection.py`: Excelente! Pool configurado, validaÃ§Ãµes, context managers
- `health_checks.py`: Completo com mÃ©tricas PostgreSQL + Redis
- Modelos bem estruturados

#### **âŒ PROBLEMAS CRÃTICOS:**

**2.1. climate_data.py (EToResults) estÃ¡ COMENTADO no __init__.py**

```python
# backend/database/models/__init__.py
# from .climate_data import EToResults  â† COMENTADO!
```

**Impacto**:
- âŒ `data_storage.py` importa `EToResults` mas ele nÃ£o estÃ¡ disponÃ­vel
- âŒ Tabela `eto_results` nÃ£o serÃ¡ criada por Alembic
- âŒ FunÃ§Ã£o `save_eto_data()` vai falhar em runtime

**2.2. data_storage.py usa modelo antigo**

O modelo `EToResults` em `climate_data.py` usa campos NASA POWER:
```python
t2m_max, t2m_min, rh2m, ws2m, radiation, precipitation
```

Mas agora temos **6 APIs diferentes** com variÃ¡veis diferentes!

**PROBLEMA**: Como salvar dados de:
- Open-Meteo (temperature_2m_max, relative_humidity_2m_mean)
- MET Norway (diferentes variÃ¡veis por regiÃ£o)
- NWS Forecast/Stations (temp_celsius, humidity_percent)
- Data Fusion (variÃ¡veis mescladas)

---

### **3. Pasta `/backend/infrastructure`**

```
backend/infrastructure/
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ celery_tasks.py        âœ… Tasks genÃ©ricas OK
â”‚   â”œâ”€â”€ climate_tasks.py       âœ… Pre-carregamento robusto
â”‚   â”œâ”€â”€ climate_cache.py       (precisa verificar)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ celery/
â”‚   â”œâ”€â”€ celery_config.py       (precisa verificar)
â”‚   â””â”€â”€ tasks/
â”‚       â”œâ”€â”€ visitor_sync.py
â”‚       â””â”€â”€ historical_download.py  âœ… NOVA task implementada
â””â”€â”€ loaders/
    â””â”€â”€ climate_history_loader.py  âš ï¸ Usa schema climate_history
```

#### **âœ… Pontos Positivos:**
- `climate_tasks.py`: 6 tasks de prÃ©-carregamento (NASA, Open-Meteo, NWS, MET Norway)
- `historical_download.py`: Task completa para downloads histÃ³ricos
- `climate_history_loader.py`: Loader completo para normais climÃ¡ticas
- **`celery_config.py`**: âœ… **EXCELENTE!** 
  - 10 tasks agendadas no Beat
  - MÃ©tricas Prometheus integradas
  - Filas separadas por tipo de task
  - MonitoredProgressTask com publicaÃ§Ã£o Redis
  - Timezone correto (America/Sao_Paulo)
- **`redis_pool.py`**: âœ… **MUITO BOM!**
  - Connection pool configurado (max 50 conexÃµes)
  - Health check interval
  - Retry on timeout
  - Decode responses habilitado
- **`session_database.py`**: âœ… **OK** - Re-export limpo

#### **âš ï¸ QuestÃµes a Resolver:**

**3.1. Schema climate_history existe?**

`climate_history_loader.py` assume schema `climate_history`:
```sql
climate_history.studied_cities
climate_history.monthly_climate_normals
```

**PERGUNTAS**:
- âœ… Schema existe? Precisa criar?
- âœ… Tabelas existem? Precisam migration?
- âœ… IntegraÃ§Ã£o com Alembic?

**3.2. Celery Beat estÃ¡ configurado?**

`climate_tasks.py` tem tasks agendadas:
- `prefetch_nasa_popular_cities`: Diariamente Ã s 03:00
- `cleanup_old_cache`: Diariamente Ã s 02:00
- `prefetch_nws_forecast_usa_cities`: A cada 6 horas
- etc.

**PERGUNTAS**:
- â“ `celery_config.py` tem configuraÃ§Ã£o de Beat?
- â“ Precisa adicionar schedule para novas tasks?

---

## ğŸ”§ **AÃ‡Ã•ES NECESSÃRIAS**

### **PRIORIDADE ALTA** (Bloqueante)

#### **1. Descomentar e Atualizar EToResults**

**Problema**: Modelo comentado no `__init__.py`

**SoluÃ§Ã£o**:
```python
# backend/database/models/__init__.py
from .climate_data import EToResults  # â† DESCOMENTAR

__all__ = [
    "AdminUser",
    "EToResults",  # â† ADICIONAR
    # ... resto
]
```

#### **2. Modernizar Modelo EToResults para Multi-API**

**OpÃ§Ã£o A - Modelo FlexÃ­vel (JSON)**:
```python
class ClimateData(Base):
    __tablename__ = "climate_data"
    
    id = Column(Integer, primary_key=True)
    source_api = Column(String(50))  # "nasa_power", "openmeteo", etc.
    latitude = Column(Float)
    longitude = Column(Float)
    date = Column(DateTime)
    
    # Dados brutos como JSON (flexÃ­vel)
    raw_data = Column(JSONB)
    
    # Dados harmonizados (mesmas unidades)
    harmonized_data = Column(JSONB)
    
    # ETo calculado
    eto_mm_day = Column(Float)
    eto_method = Column(String(20))  # "FAO-56", "Penman", etc.
```

**OpÃ§Ã£o B - Colunas EspecÃ­ficas por API** (mais complexo):
```python
# Colunas NASA POWER
t2m_max_nasa = Column(Float)
# Colunas Open-Meteo
temperature_2m_max_openmeteo = Column(Float)
# etc.
```

**RECOMENDAÃ‡ÃƒO**: **OpÃ§Ã£o A** (JSONB) por flexibilidade

#### **3. Atualizar data_storage.py**

Criar funÃ§Ã£o genÃ©rica que aceita qualquer API:
```python
def save_climate_data(
    source: str,  # "nasa_power", "data_fusion", etc.
    data: pd.DataFrame,
    lat: float,
    lon: float,
):
    """Salva dados de qualquer API."""
    for idx, row in data.iterrows():
        record = ClimateData(
            source_api=source,
            latitude=lat,
            longitude=lon,
            date=idx,
            raw_data=row.to_dict(),  # Dados brutos
            harmonized_data=harmonize(row, source),  # Normalizado
            eto_mm_day=row.get('ETo') or row.get('et0_fao_evapotranspiration'),
        )
        db.add(record)
```

#### **4. Criar MigraÃ§Ã£o Alembic**

```bash
# Criar migraÃ§Ã£o para novos modelos
alembic revision --autogenerate -m "Add climate_data and historical tables"

# Aplicar
alembic upgrade head
```

---

### **PRIORIDADE MÃ‰DIA**

#### **5. Configurar Celery Beat**

Verificar se `celery_config.py` tem:
```python
from celery.schedules import crontab

app.conf.beat_schedule = {
    'prefetch-nasa-daily': {
        'task': 'climate.prefetch_nasa_popular_cities',
        'schedule': crontab(hour=3, minute=0),  # 03:00 BRT
    },
    # ... resto das tasks
}
```

#### **6. Validar Schema climate_history**

OpÃ§Ãµes:
1. Criar via Alembic migration
2. Usar SQL direto (DDL em `database/init/`)
3. Integrar com `climate_history_loader.py`

#### **7. Implementar Task de HistÃ³rico no Beat** (se aplicÃ¡vel)

Se quiser processar downloads histÃ³ricos agendados:
```python
'process-pending-historical-downloads': {
    'task': 'backend.infrastructure.celery.tasks.process_historical_download',
    'schedule': crontab(minute='*/30'),  # A cada 30 min
},
```

---

### **PRIORIDADE BAIXA**

#### **8. Adicionar Testes de IntegraÃ§Ã£o**

Testar:
- Salvar dados de cada API
- Recuperar dados harmonizados
- Health checks
- Celery tasks

#### **9. Documentar Schema do Banco**

Criar `database/README.md` com:
- Diagrama ER
- DescriÃ§Ã£o de tabelas
- Ãndices e otimizaÃ§Ãµes
- EstratÃ©gia de particionamento (se aplicÃ¡vel)

---

## ğŸ“‹ **CHECKLIST DE IMPLEMENTAÃ‡ÃƒO**

### **Fase 1: Corrigir Modelo de Dados** (URGENTE)

- [ ] Descomentar `EToResults` em `models/__init__.py`
- [ ] Criar novo modelo `ClimateData` (JSONB flexÃ­vel)
- [ ] Atualizar `data_storage.py` para multi-API
- [ ] Criar migraÃ§Ã£o Alembic
- [ ] Testar salvamento com cada API

### **Fase 2: Validar Infraestrutura**

- [ ] âœ… **celery_config.py VALIDADO** - Excelente configuraÃ§Ã£o!
- [ ] âœ… **Celery Beat schedule COMPLETO** - 10 tasks agendadas
- [ ] âœ… **redis_pool.py VALIDADO** - Pool otimizado
- [ ] âœ… **session_database.py VALIDADO** - Re-export OK
- [ ] Validar schema `climate_history`
- [ ] Testar health checks
- [ ] Integrar nova task `process_historical_download` no Beat (se necessÃ¡rio)

### **Fase 3: IntegraÃ§Ã£o**

- [ ] Integrar `historical_download` task com salvamento BD
- [ ] Testar workflow completo: requisiÃ§Ã£o â†’ download â†’ save â†’ email
- [ ] Validar tasks de prÃ©-carregamento
- [ ] Monitorar mÃ©tricas Prometheus

### **Fase 4: Testes**

- [ ] Testes unitÃ¡rios para modelos
- [ ] Testes de integraÃ§Ã£o para tasks
- [ ] Testes de carga (stress test)
- [ ] ValidaÃ§Ã£o de seguranÃ§a (SQL injection, etc.)

---

## ğŸ¯ **PRÃ“XIMOS PASSOS SUGERIDOS**

### **Imediato** (hoje):
1. âœ… Descomentar `EToResults` (1 linha)
2. âœ… Criar modelo `ClimateData` (novo arquivo)
3. âœ… Atualizar `data_storage.py` (funÃ§Ã£o genÃ©rica)
4. âœ… Rodar migraÃ§Ã£o Alembic

### **Curto Prazo** (esta semana):
5. âœ… Validar Celery Beat config
6. âœ… Testar workflow de download histÃ³rico
7. âœ… Verificar schema `climate_history`

### **MÃ©dio Prazo** (prÃ³ximas 2 semanas):
8. âœ… Testes de integraÃ§Ã£o
9. âœ… DocumentaÃ§Ã£o completa
10. âœ… Deploy em staging

---

## âš ï¸ **AVISOS IMPORTANTES**

### **1. MigraÃ§Ã£o de Dados**

Se jÃ¡ existem dados em `eto_results` (modelo antigo):
- âš ï¸ Criar script de migraÃ§Ã£o
- âš ï¸ Backup antes de alterar schema
- âš ï¸ Testar migraÃ§Ã£o em ambiente de dev

### **2. Compatibilidade**

Garantir que cÃ³digo legado que usa `EToResults` continue funcionando:
- âœ… Manter modelo antigo (deprecated)
- âœ… Criar adapter/wrapper
- âœ… Documentar migraÃ§Ã£o

### **3. Performance**

JSONB pode ter impacto:
- âœ… Criar Ã­ndices GIN para queries em JSON
- âœ… Considerar particionamento por data
- âœ… Monitorar query performance

---

## ğŸ“Š **RESUMO EXECUTIVO**

| Componente | Status | AÃ§Ã£o |
|------------|--------|------|
| **database/config** | âœ… OK | Nenhuma |
| **database/init** | âš ï¸ Incompleto | Melhorar init_alembic.py |
| **backend/database/connection** | âœ… Excelente | Nenhuma |
| **backend/database/redis_pool** | âœ… Muito Bom | Nenhuma |
| **backend/database/session_database** | âœ… OK | Nenhuma |
| **backend/database/models** | âŒ Problema | Descomentar + modernizar |
| **backend/database/data_storage** | âš ï¸ Desatualizado | Reescrever para multi-API |
| **backend/database/health_checks** | âœ… Completo | Nenhuma |
| **infrastructure/cache/celery_tasks** | âœ… Bom | Nenhuma |
| **infrastructure/cache/climate_tasks** | âœ… Excelente | Nenhuma |
| **infrastructure/celery/celery_config** | âœ… Excelente | Nenhuma |
| **infrastructure/celery/tasks** | âœ… Organizado | Nenhuma |
| **infrastructure/loaders** | âš ï¸ Schema? | Validar climate_history |

### **Score Geral**: **8/10** â¬†ï¸ (melhorou de 7/10)

**CrÃ­tico**: Modelo `EToResults` comentado e desatualizado  
**Excelente**: Celery config completo + Beat schedule + Redis pool  
**Muito Bom**: Health checks, connection pool, tasks de prÃ©-carregamento  

---

## ğŸ“Š **ANÃLISE DETALHADA - NOVOS ARQUIVOS**

### **âœ… celery_config.py - CONFIGURAÃ‡ÃƒO PERFEITA**

**Pontos Fortes**:
1. âœ… **Beat Schedule Completo**: 10 tasks agendadas
   - Limpeza de cache (02:00)
   - Pre-fetch NASA (03:00)
   - Pre-fetch NWS Forecast (a cada 6h)
   - Pre-fetch NWS Stations (04:00)
   - Pre-fetch Open-Meteo Forecast (05:00)
   - Pre-fetch Open-Meteo Archive (domingo 06:00)
   - Pre-fetch MET Norway (07:00)
   - Stats de cache (a cada hora)
   - Sync visitantes (a cada 30min)
   - Limpeza expirados (meia-noite)

2. âœ… **Filas Separadas**:
   - `general`: Tasks genÃ©ricas
   - `eto_processing`: CÃ¡lculos de ETo
   - `data_download`: Downloads climÃ¡ticos
   - `data_processing`: Processamento de dados
   - `elevation`: ServiÃ§os de elevaÃ§Ã£o

3. âœ… **MonitoredProgressTask**:
   - MÃ©tricas Prometheus automÃ¡ticas
   - PublicaÃ§Ã£o de progresso via Redis/WebSocket
   - Rastreamento de duraÃ§Ã£o e status

4. âœ… **Timezone Correto**: `America/Sao_Paulo`

**Ãšnica ObservaÃ§Ã£o**:
- â³ Task `process_historical_download` **NÃƒO estÃ¡ no Beat**
- Isso estÃ¡ OK! Task Ã© acionada sob demanda (`.delay()`)
- NÃ£o precisa agendamento automÃ¡tico

### **âœ… redis_pool.py - CONNECTION POOL OTIMIZADO**

**ConfiguraÃ§Ãµes**:
```python
max_connections=50          # Limite de conexÃµes
socket_timeout=10           # Timeout de operaÃ§Ãµes
socket_connect_timeout=5    # Timeout de conexÃ£o
retry_on_timeout=True       # Retry automÃ¡tico
health_check_interval=30    # Ping a cada 30s
decode_responses=True       # Strings em vez de bytes
```

**PadrÃ£o Singleton**: Garante pool Ãºnico global

### **âœ… session_database.py - RE-EXPORT LIMPO**

FunÃ§Ã£o: Compatibilidade com cÃ³digo legado
- Re-exporta `get_db`, `get_db_context`, `engine`, `Base`
- MantÃ©m imports funcionando

---

**Quer que eu comece pelas correÃ§Ãµes crÃ­ticas (Fase 1)?** ğŸ”§
